{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91e3fc8d",
   "metadata": {},
   "source": [
    "Las herramientas y recursos pueden no estar ubicados en tu máquina local, pueden estar en cualquier lugar de Internet; solo necesitamos una forma estándar de comunicarnos con esos puntos de entrada.\n",
    "\n",
    "# Protocolo de Contexto de Modelo\n",
    "\n",
    "El Protocolo de Contexto de Modelo (MCP, por sus siglas en inglés) es un estándar abierto diseñado para facilitar la comunicación fluida entre modelos de aprendizaje automático, herramientas y recursos, sin importar su ubicación física. MCP define un conjunto de convenciones y APIs que permiten la interoperabilidad, haciendo más fácil integrar modelos y servicios a través de diversas plataformas y entornos.\n",
    "\n",
    "Las características clave de MCP incluyen:\n",
    "- **APIs estandarizadas** para la invocación de modelos e intercambio de contexto\n",
    "- **Soporte para recursos remotos y distribuidos**\n",
    "- **Extensibilidad** para acomodar varios tipos de modelos y flujos de trabajo\n",
    "\n",
    "Para información más detallada, consulta la [Especificación del Protocolo de Contexto de Modelo](https://github.com/modelcontext/protocol) y la [documentación oficial](https://modelcontext.org/docs/protocol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb0514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_server.py\n",
    "from random import random\n",
    "from fastmcp import FastMCP\n",
    "\n",
    "# Instrucciones sobre como actuar\n",
    "mcp = FastMCP(\n",
    "    name=\"Asistente\",\n",
    "    instructions=\"\"\"\n",
    "        Eres un asistente de ayuda.\n",
    "        La función multiply() permite multiplicar dos valores.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "@mcp.tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiplies two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@mcp.resource(\"data://config\")\n",
    "def config() -> dict:\n",
    "    \"\"\"MCP server configuration\"\"\"\n",
    "    return {\"verbose\" : True, \"seed\" : 1234}\n",
    "\n",
    "@mcp.prompt\n",
    "def simple_prompt(data_points: list[float]) -> str:\n",
    "    \"\"\"Creates a prompt asking help on some data points\"\"\"\n",
    "    formatted_data = \", \".join(str(point) for point in data_points)\n",
    "    return f\"Please analyze these data points: {formatted_data}\"\n",
    "\n",
    "# Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7001b152",
   "metadata": {},
   "source": [
    "Ahora, puedes ejecutar el servidor en un terminal.\n",
    "\n",
    "```\n",
    "fastmcp run my_server.py:mcp\n",
    "```\n",
    "\n",
    "con el entornos activado o poniendo delante `uv run` para permitir que uv gestione las dependencias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f464d",
   "metadata": {},
   "source": [
    "## Herramientas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9465e828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: multiply\n",
      "Description: Multiplies two numbers together.\n",
      "Parameters: {'properties': {'a': {'title': 'A', 'type': 'number'}, 'b': {'title': 'B', 'type': 'number'}}, 'required': ['a', 'b'], 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "from fastmcp import Client\n",
    "\n",
    "client = Client(\"my_server.py\")\n",
    "\n",
    "async with client:\n",
    "    tools = await client.list_tools()\n",
    "    \n",
    "    for tool in tools:\n",
    "        print(f\"Tool: {tool.name}\")\n",
    "        print(f\"Description: {tool.description}\")\n",
    "        if tool.inputSchema:\n",
    "            print(f\"Parameters: {tool.inputSchema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de015f16",
   "metadata": {},
   "source": [
    "## Recursos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "257da19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource URI: data://config\n",
      "Name: config\n",
      "Description: MCP server configuration\n",
      "MIME Type: text/plain\n"
     ]
    }
   ],
   "source": [
    "async with client:\n",
    "    resources = await client.list_resources()\n",
    "    \n",
    "    for resource in resources:\n",
    "        print(f\"Resource URI: {resource.uri}\")\n",
    "        print(f\"Name: {resource.name}\")\n",
    "        print(f\"Description: {resource.description}\")\n",
    "        print(f\"MIME Type: {resource.mimeType}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5c0e6a",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82017e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: simple_prompt\n",
      "Description: Creates a prompt asking help on some data points\n",
      "Arguments: ['data_points']\n"
     ]
    }
   ],
   "source": [
    "async with client:\n",
    "    prompts = await client.list_prompts()\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt.name}\")\n",
    "        print(f\"Description: {prompt.description}\")\n",
    "        if prompt.arguments:\n",
    "            print(f\"Arguments: {[arg.name for arg in prompt.arguments]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaad1ee",
   "metadata": {},
   "source": [
    "## LLamadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a5b2397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CallToolResult(content=[TextContent(type='text', text='40.0', annotations=None, meta=None)], structured_content={'result': 40.0}, data=40.0, is_error=False)\n"
     ]
    }
   ],
   "source": [
    "async with client:\n",
    "    result = await client.call_tool(\"multiply\", {\"a\": 4, \"b\" : 10})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7b5226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta=None description='Creates a prompt asking help on some data points' messages=[PromptMessage(role='user', content=TextContent(type='text', text='Please analyze these data points: 1.0, 2.0, 3.0', annotations=None, meta=None))]\n"
     ]
    }
   ],
   "source": [
    "async with client:\n",
    "    result = await client.get_prompt(\"simple_prompt\", {\"data_points\": [1, 2, 3]})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49fc7816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextResourceContents(uri=AnyUrl('data://config'), mimeType='text/plain', meta=None, text='{\"verbose\":true,\"seed\":1234}')]\n"
     ]
    }
   ],
   "source": [
    "async with client:\n",
    "    number = await client.read_resource(\"data://config\")\n",
    "    print(number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53daa6",
   "metadata": {},
   "source": [
    "## Servidores MCP externos\n",
    "\n",
    "Existen muchas aplicaciones que ofrecen servidores MCP externos, además de la gran cantidad de servidores API disponibles para casi todas las soluciones de productividad.\n",
    "\n",
    "https://mcpservers.org/remote-mcp-servers\n",
    "\n",
    "Puedes probar con el servidor MCP de Github, por ejemplo https://api.githubcopilot.com/mcp/ o crear un MCP desde cualquier repositorio con https://gitmcp.io/ o una instancia de Github Pages. Por ejemplo, tomemos la documentación de LangChain.\n",
    "\n",
    "https://langchain-ai.github.io/langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ec39befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown SSE event: endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: fetch_langchain_documentation\n",
      "Description: Fetch entire documentation file from the langchain-ai/langchain GitHub Pages. Useful for general questions. Always call this tool first if asked about langchain-ai/langchain.\n",
      "Parameters: {'type': 'object'}\n",
      "Tool: search_langchain_documentation\n",
      "Description: Semantically search within the fetched documentation from the langchain-ai/langchain GitHub Pages. Useful for specific queries.\n",
      "Parameters: {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'The search query to find relevant documentation'}}, 'required': ['query'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}\n",
      "Tool: search_langchain_code\n",
      "Description: Search for code within the GitHub repository: \"langchain-ai/langchain\" using the GitHub Search API (exact match). Returns matching files for you to query further if relevant.\n",
      "Parameters: {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'The search query to find relevant code files'}, 'page': {'type': 'number', 'description': 'Page number to retrieve (starting from 1). Each page contains 30 results.'}}, 'required': ['query'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}\n",
      "Tool: fetch_generic_url_content\n",
      "Description: Generic tool to fetch content from any absolute URL, respecting robots.txt rules. Use this to retrieve referenced urls (absolute urls) that were mentioned in previously fetched documentation.\n",
      "Parameters: {'type': 'object', 'properties': {'url': {'type': 'string', 'description': 'The URL of the document or page to fetch'}}, 'required': ['url'], 'additionalProperties': False, '$schema': 'http://json-schema.org/draft-07/schema#'}\n"
     ]
    }
   ],
   "source": [
    "from fastmcp import Client\n",
    "\n",
    "# We need to change github for gitmcp\n",
    "client = Client(f\"https://langchain-ai.gitmcp.io/langchain\")\n",
    "\n",
    "async with client:\n",
    "    tools = await client.list_tools()\n",
    "    \n",
    "    for tool in tools:\n",
    "        print(f\"Tool: {tool.name}\")\n",
    "        print(f\"Description: {tool.description}\")\n",
    "        if tool.inputSchema:\n",
    "            print(f\"Parameters: {tool.inputSchema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fcafa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown SSE event: endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextContent(type='text', text='## Query\\n\\nmemory.\\n\\n## Response\\n\\n### Sources:\\nImportant: you can fetch the full content of any source using the fetch_url_content tool\\n\\n#### (langchain-ai/langchain/docs/docs/versions/migrating_memory/index.mdx)[https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/versions/migrating_memory/index.mdx] (Score: 0.54)\\n- \\n\\nPlease see [long-term memory agent tutorial](long_term_memory_agent) implements an agent that can extract structured information from the conversation history.\\n\\nMemory classes that fall into this category include:\\n\\n| Memory Type                | Description                                                                                                                                                                                                       |\\n|----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `BaseEntityStore`          | An abstract interface that resembles a key-value store. It was used for storing structured information learned during the conversation. The information had to be represented as a dictionary of key-value pairs. |\\n| `ConversationEntityMemory` | Combines the ability to summarize the conversation while extracting structured information from the conversation history.                                                                                         |\\n\\nAnd specific backend implementations of abstractions:\\n\\n| Memory Type               | Description                                                                                              |\\n|---------------------------|----------------------------------------------------------------------------------------------------------|\\n| `InMemoryEntityStore`     | An implementation of `BaseEntityStore` that stores the information in the literal computer memory (RAM). |\\n| `RedisEntityStore`        | A specific implementation of `BaseEntityStore` that uses Redis as the backend.                           |\\n| `SQLiteEntityStore`       | A specific implementation of `BaseEntityStore` that uses SQLite as the backend.                          |\\n| `UpstashRedisEntityStore` | A specific implementation of `BaseEntityStore` that uses Upstash as the backend.         \\n- \\n\\nMemory classes that fall into this category include:\\n\\n| Memory Type            | Description                                                                                                                    |\\n|------------------------|--------------------------------------------------------------------------------------------------------------------------------|\\n| `CombinedMemory`       | This abstraction accepted a list of `BaseMemory` and fetched relevant memory information from each of them based on the input. |\\n| `SimpleMemory`         | Used to add read-only hard-coded context. Users can simply write this information into the prompt.                             |\\n| `ReadOnlySharedMemory` | Provided a read-only view of an existing `BaseMemory` implementation.                                                          |\\n\\nThese implementations did not seem to be used widely or provide significant value. Users should be able\\nto re-implement these without too much difficulty in custom code.\\n\\n#\\n- \\n\\nThe concept of memory has evolved significantly in LangChain since its initial release.\\n\\n#\\n- \\n\\nThe goal of managing conversation history is to store and retrieve the history in a way that is optimal for a chat model to use.\\n\\nOften this involves trimming and / or summarizing the conversation history to keep the most relevant parts of the conversation while having the conversation fit inside the context window of the chat model.\\n\\nMemory classes that fall into this category include:\\n\\n| Memory Type                       | How to Migrate                                                                                                                                              | Description                                                                                                                                                                                                         |\\n|-----------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\\n| `ConversationBufferMemory`        | [Link to Migration Guide](conversation_buffer_memory)                                                                                                       | A basic memory implementation that simply stores the conversation history.                                                                                                                                          |\\n| `ConversationStringBufferMemory`  | [Link to Migration Guide](conversation_buffer_memory)                                                                                                       | A special case of `ConversationBufferMemory` designed for LLMs and no longer relevant.                                                                                                                              |\\n| `ConversationB\\n- eRetrieverMemory`      | See related [long-term memory agent tutorial](long_term_memory_agent) | Stores the conversation history in a vector store and retrieves the most relevant parts of past conversation based on the input.                                                                                    |\\n\\n\\n#\\n- \\n\\nBroadly speaking, LangChain 0.0.x memory was used to handle three main use cases:\\n\\n| Use Case                             | Example                                                                                                                           |\\n|--------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|\\n| Managing conversation history        | Keep only the last `n` turns of the conversation between the user and the AI.                                                     |\\n| Extraction of structured information | Extract structured information from the conversation history, such as a list of facts learned about the user.                     |\\n| Composite memory implementations     | Combine multiple memory sources, e.g., a list of known facts about the user along with facts learned during a given conversation. |\\n\\nWhile the LangChain 0.0.x memory abstractions were useful, they were limited in their capabilities and not well suited for real-world conversational AI applications. These memory abstractions lacked built-in support for multi-user, multi-conversation scenarios, which are essential for practical conversational AI systems.\\n\\nMost of these implementations have been officially deprecated in LangChain 0.3.x in favor of LangGraph persistence.\\n\\n#\\n\\n#### (langchain-ai/langchain/docs/docs/integrations/providers/remembrall.mdx)[https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/integrations/providers/remembrall.mdx] (Score: 0.53)\\n- \\n\\n>[Remembrall](https://remembrall.dev/) is a platform that gives a language model \\n> long-term memory, retrieval augmented generation, and complete observability.\\n \\n#\\n- \\n\\nSee a [usage example](/docs/integrations/memory/remembrall)\\n\\n#### (langchain-ai/langchain/docs/docs/integrations/providers/redis.mdx)[https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/integrations/providers/redis.mdx] (Score: 0.51)\\n- he\\nSemantic caching allows users to retrieve cached prompts based on semantic similarity between the user input and previously cached results. Under the hood it blends Redis as both a cache and a vectorstore.\\n\\n```python\\nfrom langchain.cache import RedisSemanticCache\\n```\\n\\nTo use this cache with your LLMs:\\n```python\\nfrom langchain.globals import set_llm_cache\\nimport re\\n\\n#### (langchain-ai/langchain/docs/docs/integrations/providers/zep.mdx)[https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/integrations/providers/zep.mdx] (Score: 0.51)\\n- \\n> Recall, understand, and extract data from chat histories. Power personalized AI experiences.\\n\\n>[Zep](https://www.getzep.com) is a long-term memory service for AI Assistant apps.\\n> With Zep, you can provide AI assistants with the ability to recall past conversations, no matter how distant,\\n> while also reducing hallucinations, latency, and cost.\\n\\n#\\n\\n#### (langchain-ai/langchain/docs/docs/integrations/providers/couchbase.mdx)[https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/integrations/providers/couchbase.mdx] (Score: 0.49)\\n- \\nSemantic caching allows users to retrieve cached prompts based on the semantic similarity between the user input and previously cached inputs. Under the hood it uses Couchbase as both a cache and a vectorstore.\\nThe CouchbaseSemanticCache needs a Search Index defined to work. Please look at the [usage example](/docs/integrations/vectorstores/couchbase) on how to set up the index.\\n\\nSee a [usage example](/docs/integrations/llm_caching/#couchbase-caches).\\n\\nTo import this cache:\\n```python\\nfrom langchain_couchbase.cache import CouchbaseSemanticCache\\n```\\n\\nTo use this cache with your LLMs:\\n```python\\nfrom langchain_core.globals import set_llm_cach\\n\\n#### (langchain-ai/langchain/docs/docs/concepts/vectorstores.mdx)[https://raw.githubusercontent.com/langchain-ai/langchain/master/docs/docs/concepts/vectorstores.mdx] (Score: 0.49)\\n- id search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |\\n| [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search) | When needing to diversify search results.             | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.                                  \\n', annotations=None, meta=None)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "async with client:\n",
    "    result = await client.call_tool(\"search_langchain_documentation\", {\"query\" : \"memory\"})\n",
    "    pprint.pprint(result.content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025453fa",
   "metadata": {},
   "source": [
    "De manera similar, puedes añadir esta conexión a tu asistente de codificación para que pueda realizar las mismas acciones (consultar herramientas, recursos o lo que exponga el servidor MCP).\n",
    "\n",
    "```\n",
    "{\n",
    "  \"servers\": {\n",
    "    \"langchain Docs\": {\n",
    "      \"type\": \"sse\",\n",
    "      \"url\": \"https://langchain-ai.gitmcp.io/langchain\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
